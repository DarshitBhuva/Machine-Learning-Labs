{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+emifQWrhoFwylNFvJ9X/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://www.analyticsvidhya.com/blog/2021/08/linear-regression-and-gradient-descent-in-pytorch/\n"],"metadata":{"id":"Wj2hUHR9mFP7"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"QEOB4ZcggEP8","executionInfo":{"status":"ok","timestamp":1674464190788,"user_tz":-330,"elapsed":3911,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}}},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43],\n","                    [91, 88, 64],\n","                    [87, 134, 58],\n","                    [102, 43, 37],\n","                    [69, 96, 70]], dtype='float32')\n","\n","targets = np.array([[56],\n","          [81],\n","          [119],\n","          [22],\n","          [103]], dtype='float32')"],"metadata":{"id":"I5ZPglNBgeIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert inputs and targets to tensors\n","input = torch.from_numpy(inputs)\n","target = torch.from_numpy(targets)\n","\n","\n","print(input)\n","print(target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4IEd1Y9iALt","executionInfo":{"status":"ok","timestamp":1672048073090,"user_tz":-330,"elapsed":18,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"e5bca215-c540-4c54-f238-aedda2c2ac81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]])\n","tensor([[ 56.],\n","        [ 81.],\n","        [119.],\n","        [ 22.],\n","        [103.]])\n"]}]},{"cell_type":"code","source":["from torch.utils import data\n","from torch.utils.data import TensorDataset\n","dataset = TensorDataset(input, target)\n","\n","print(dataset[:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3w3VUXdlFFV","executionInfo":{"status":"ok","timestamp":1672048073091,"user_tz":-330,"elapsed":17,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"a0b01483-1728-44b5-dced-182e80486fa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]]), tensor([[ 56.],\n","        [ 81.],\n","        [119.],\n","        [ 22.],\n","        [103.]]))\n"]}]},{"cell_type":"markdown","source":["Using Pytorch’s DataLoader class we can convert the dataset into batches of predefined batch size and create batches by picking samples from the dataset randomly."],"metadata":{"id":"nQx2C7aHpRCE"}},{"cell_type":"code","source":["\n","from torch.utils.data import DataLoader\n","\n","batch_size = 5\n","train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"PGksuwiPopxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inp,target in train_loader:\n","    print(inp)\n","    print(target)\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxL5sr34oxkb","executionInfo":{"status":"ok","timestamp":1672048073092,"user_tz":-330,"elapsed":16,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"abfac5f9-d4bc-45e0-fd22-15f4c18811d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 69.,  96.,  70.],\n","        [ 87., 134.,  58.],\n","        [ 91.,  88.,  64.],\n","        [ 73.,  67.,  43.],\n","        [102.,  43.,  37.]])\n","tensor([[103.],\n","        [119.],\n","        [ 81.],\n","        [ 56.],\n","        [ 22.]])\n"]}]},{"cell_type":"code","source":["w = torch.randn(1, 3, requires_grad=True)\n","b = torch.randn(5,1, requires_grad=True)\n","print(w)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jm7gr5_krauS","executionInfo":{"status":"ok","timestamp":1674464203502,"user_tz":-330,"elapsed":407,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"bc5b9b3a-8a53-48d6-8b21-09afbccfb944"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1025, -1.0508, -0.8750]], requires_grad=True)\n","tensor([[-1.1160],\n","        [-0.8317],\n","        [ 1.1303],\n","        [ 0.2498],\n","        [ 0.0714]], requires_grad=True)\n"]}]},{"cell_type":"code","source":["def model(X):\n","    return X @ w.t() + b\n","\n","def mse_loss(predictions, targets):\n","    difference = predictions - targets\n","    return torch.sum(difference * difference)/(2*difference.numel())\n","    \n","# numel() method returns the number of elements in the tensor.\n","\n","for x,y in train_loader:\n","    preds = model(x)\n","    print(\"Prediction is :n\",preds)\n","    print(\"nActual targets is :n\",y)\n","    \n","    print(\"Loss is : \", mse_loss(preds, y))\n","\n","  \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7FsS2xZsY5k","executionInfo":{"status":"ok","timestamp":1672048162751,"user_tz":-330,"elapsed":499,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"24b2afe6-d01c-476f-f531-b8c8051c8344"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction is :n tensor([[351.5450],\n","        [275.3419],\n","        [339.7699],\n","        [267.4523],\n","        [334.3695]], grad_fn=<AddBackward0>)\n","nActual targets is :n tensor([[ 22.],\n","        [103.],\n","        [ 81.],\n","        [ 56.],\n","        [119.]])\n","Loss is :  tensor(29635.9570, grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["epochs = 100\n","for i in range(epochs):\n","    # Iterate through training dataloader\n","    for x,y in train_loader:\n","        # Generate Prediction\n","        preds = model(x)\n","        # Get the loss and perform backpropagation\n","        loss = mse_loss(preds, y)\n","        loss.backward()\n","        # Let's update the weights\n","        with torch.no_grad():\n","            w -= w.grad *1e-6\n","            b -= b.grad * 1e-6\n","            # Set the gradients to zero\n","            w.grad.zero_()\n","            b.grad.zero_()\n","    print(f\"Epoch {i}/{epochs}: Loss: {loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUZF6JOIyg-5","executionInfo":{"status":"ok","timestamp":1672048209071,"user_tz":-330,"elapsed":421,"user":{"displayName":"CE023_Darshit_ Bhuva","userId":"06524650222381384026"}},"outputId":"3a4fc218-2596-40c2-bc91-c80692d52373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/100: Loss: 3341.735595703125\n","Epoch 1/100: Loss: 3354.107421875\n","Epoch 2/100: Loss: 3384.94189453125\n","Epoch 3/100: Loss: 3269.568115234375\n","Epoch 4/100: Loss: 3235.3759765625\n","Epoch 5/100: Loss: 3248.0693359375\n","Epoch 6/100: Loss: 3201.34521484375\n","Epoch 7/100: Loss: 3135.31396484375\n","Epoch 8/100: Loss: 3195.870361328125\n","Epoch 9/100: Loss: 3190.156982421875\n","Epoch 10/100: Loss: 3124.74609375\n","Epoch 11/100: Loss: 3135.96337890625\n","Epoch 12/100: Loss: 3149.727294921875\n","Epoch 13/100: Loss: 3044.2255859375\n","Epoch 14/100: Loss: 3088.02197265625\n","Epoch 15/100: Loss: 3027.77783203125\n","Epoch 16/100: Loss: 2977.29345703125\n","Epoch 17/100: Loss: 2936.03515625\n","Epoch 18/100: Loss: 2998.971435546875\n","Epoch 19/100: Loss: 2927.34912109375\n","Epoch 20/100: Loss: 2910.34033203125\n","Epoch 21/100: Loss: 2992.80810546875\n","Epoch 22/100: Loss: 2851.735107421875\n","Epoch 23/100: Loss: 2897.646484375\n","Epoch 24/100: Loss: 2886.947998046875\n","Epoch 25/100: Loss: 2844.12060546875\n","Epoch 26/100: Loss: 2795.65478515625\n","Epoch 27/100: Loss: 2805.8486328125\n","Epoch 28/100: Loss: 2862.440185546875\n","Epoch 29/100: Loss: 2882.638916015625\n","Epoch 30/100: Loss: 2768.43505859375\n","Epoch 31/100: Loss: 2796.89697265625\n","Epoch 32/100: Loss: 2747.641357421875\n","Epoch 33/100: Loss: 2767.14306640625\n","Epoch 34/100: Loss: 2791.45703125\n","Epoch 35/100: Loss: 2780.972412109375\n","Epoch 36/100: Loss: 2703.86279296875\n","Epoch 37/100: Loss: 2737.635009765625\n","Epoch 38/100: Loss: 2686.40673828125\n","Epoch 39/100: Loss: 2712.460205078125\n","Epoch 40/100: Loss: 2640.539306640625\n","Epoch 41/100: Loss: 2630.43994140625\n","Epoch 42/100: Loss: 2664.503173828125\n","Epoch 43/100: Loss: 2640.795166015625\n","Epoch 44/100: Loss: 2642.64501953125\n","Epoch 45/100: Loss: 2680.896728515625\n","Epoch 46/100: Loss: 2622.08154296875\n","Epoch 47/100: Loss: 2608.796142578125\n","Epoch 48/100: Loss: 2638.94873046875\n","Epoch 49/100: Loss: 2654.439453125\n","Epoch 50/100: Loss: 2658.635986328125\n","Epoch 51/100: Loss: 2591.082763671875\n","Epoch 52/100: Loss: 2579.414794921875\n","Epoch 53/100: Loss: 2564.4765625\n","Epoch 54/100: Loss: 2590.948974609375\n","Epoch 55/100: Loss: 2606.888671875\n","Epoch 56/100: Loss: 2600.293212890625\n","Epoch 57/100: Loss: 2542.80615234375\n","Epoch 58/100: Loss: 2589.74169921875\n","Epoch 59/100: Loss: 2621.70263671875\n","Epoch 60/100: Loss: 2525.7861328125\n","Epoch 61/100: Loss: 2569.10009765625\n","Epoch 62/100: Loss: 2568.207763671875\n","Epoch 63/100: Loss: 2510.366943359375\n","Epoch 64/100: Loss: 2532.67822265625\n","Epoch 65/100: Loss: 2505.517578125\n","Epoch 66/100: Loss: 2511.4912109375\n","Epoch 67/100: Loss: 2479.451171875\n","Epoch 68/100: Loss: 2536.13671875\n","Epoch 69/100: Loss: 2509.41357421875\n","Epoch 70/100: Loss: 2490.24267578125\n","Epoch 71/100: Loss: 2493.663818359375\n","Epoch 72/100: Loss: 2494.182861328125\n","Epoch 73/100: Loss: 2446.46875\n","Epoch 74/100: Loss: 2473.90576171875\n","Epoch 75/100: Loss: 2508.65869140625\n","Epoch 76/100: Loss: 2440.528564453125\n","Epoch 77/100: Loss: 2466.4775390625\n","Epoch 78/100: Loss: 2433.583740234375\n","Epoch 79/100: Loss: 2516.763671875\n","Epoch 80/100: Loss: 2451.175048828125\n","Epoch 81/100: Loss: 2420.310546875\n","Epoch 82/100: Loss: 2407.47119140625\n","Epoch 83/100: Loss: 2461.023681640625\n","Epoch 84/100: Loss: 2432.010986328125\n","Epoch 85/100: Loss: 2466.49658203125\n","Epoch 86/100: Loss: 2485.9072265625\n","Epoch 87/100: Loss: 2357.38623046875\n","Epoch 88/100: Loss: 2451.4052734375\n","Epoch 89/100: Loss: 2351.534423828125\n","Epoch 90/100: Loss: 2380.655517578125\n","Epoch 91/100: Loss: 2383.746826171875\n","Epoch 92/100: Loss: 2425.519287109375\n","Epoch 93/100: Loss: 2333.635009765625\n","Epoch 94/100: Loss: 2427.692138671875\n","Epoch 95/100: Loss: 2384.39306640625\n","Epoch 96/100: Loss: 2354.51904296875\n","Epoch 97/100: Loss: 2406.40087890625\n","Epoch 98/100: Loss: 2403.337646484375\n","Epoch 99/100: Loss: 2405.3203125\n"]}]}]}